{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8faa87c2",
   "metadata": {},
   "source": [
    "# Korean Sign Language Recognizer\n",
    "##### 프로젝트 개요 작성하면 딱인 위치"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81445769",
   "metadata": {},
   "source": [
    "## 1. 데이터셋 생성\n",
    "\n",
    "기존에 인터넷에서 찾아볼 수 있는 자료는 영상 또는 해외 수어기 때문에 한국 수어 지문자 이미지 데이터셋을 자체 제작함\n",
    "\n",
    "총 26종, 3,153개 이미지 데이터 생성\n",
    "\n",
    "![ksl](https://mblogthumb-phinf.pstatic.net/20131030_109/souldeaf_1383105936536OX6gP_JPEG/%C1%F6%B9%AE%C0%DA2.JPG?type=w2)\n",
    "이미지 제작에 참고한 지문자 목록 / https://mblogthumb-phinf.pstatic.net/20131030_109/souldeaf_1383105936536OX6gP_JPEG/%C1%F6%B9%AE%C0%DA2.JPG?type=w2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a371583",
   "metadata": {},
   "source": [
    "> captureROI.py  \n",
    "\n",
    "cv2를 사용해 지문자 이미지 캡쳐"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b358fb75",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "from glob import glob\n",
    "\n",
    "cap = cv2.VideoCapture(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "385fe7a8",
   "metadata": {},
   "source": [
    "사용할 모듈을 가져오고 0번 카메라를 사용하는 VideoCapture 객체를 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a467ddb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "\n",
    "# 저장 폴더\n",
    "base_dir = \"./captures\"\n",
    "if not os.path.exists(base_dir):\n",
    "    os.mkdir(base_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d1e0592",
   "metadata": {},
   "source": [
    "기본적인 변수를 초기화하고, 데이터를 저장할 폴더가 없을 시 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "97865a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def saveROI(base, target, frame):\n",
    "    \"\"\"인수로 받은 폴더에 프레임 저장\n",
    "\n",
    "    Args:\n",
    "        base (str): 루트 폴더\n",
    "        target (str): 저장하고자하는 폴더\n",
    "        frame (numpy.ndarray): 저장할 이미지\n",
    "    \"\"\"\n",
    "    if not os.path.exists(os.path.join(base, target)):\n",
    "        os.mkdir(os.path.join(base, target))\n",
    "\n",
    "    # 폴더 내 png 파일의 수로 파일 이름 지정\n",
    "    print(os.path.join(base, target, \"*.png\"))\n",
    "    imglist = glob(os.path.join(base, target, \"*.png\"))\n",
    "    count = len(imglist)\n",
    "\n",
    "    w = cv2.imwrite(os.path.join(base, target, f\"{count + 1}.png\"), frame)  # 경로에 한글 포함 시 저장 x\n",
    "    # status\n",
    "    print(w)\n",
    "    print(os.path.join(base, target, f\"{count + 1}.png saved\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff0801b8",
   "metadata": {},
   "source": [
    "루트 폴더와 해당 지문자, 이미지를 인수로 받아 루트 폴더 내 해당 지문자 폴더에 이미지를 1부터 순서대로 저장함\n",
    "\n",
    "cv2의 경우 한글을 지원하지 않기 때문에 base, target에 한글이 포함되면 오류 발생"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6b28074a",
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    # 관심 영역 추출 및 표시\n",
    "    roi = frame[100:300, 200:400].copy()\n",
    "    frame = cv2.rectangle(frame, (200, 100), (400, 300), (255, 0, 0), 5, cv2.LINE_8)\n",
    "\n",
    "    # 관심 영역 크기 조절\n",
    "    roi = cv2.resize(roi, dsize=(100, 100), interpolation=cv2.INTER_LINEAR)\n",
    "\n",
    "    # 25fps로 화면에 출력\n",
    "    cv2.imshow(\"frame\", frame)\n",
    "    # cv2.imshow(\"roi\", roi)\n",
    "    key = cv2.waitKey(40)\n",
    "\n",
    "    # esc 입력 시 종료\n",
    "    if key == 27:\n",
    "        break\n",
    "\n",
    "    elif (key >= 97 and key <= 122) or (key >= 48 and key <= 57):\n",
    "        saveROI(base_dir, chr(key), roi)\n",
    "        \n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41bbd884",
   "metadata": {},
   "source": [
    "구역을 특정하고 a-z, 0-9 사이의 키를 입력했을 시 해당 구역의 이미지를 저장\n",
    "\n",
    "저장할 때 경로는 \"위에서 지정한 base_dir/입력한 키/숫자.png\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d99e0f11",
   "metadata": {},
   "source": [
    "##### 데이터셋 만드는거 캡처해서 넣으면 딱인 위치"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f2090f5",
   "metadata": {},
   "source": [
    "> renameFolders.py\n",
    "\n",
    "cv2에서는 한글을 사용할 수 없기 때문에 폴더 이름을 한글 키보드 위치에 해당하는 영어로 저장\n",
    "\n",
    "영어를 해당하는 지문자로 변경하기 위한 코드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1d425359",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'r': 'ㄱ', 'R': 'ㄲ', 'rt': 'ㄳ', 's': 'ㄴ', 'sw': 'ㄵ', 'sg': 'ㄶ', 'e': 'ㄷ', 'f': 'ㄹ', 'fr': 'ㄺ', 'fa': 'ㄻ', 'fq': 'ㄼ', 'ft': 'ㄽ', 'fx': 'ㄾ', 'fv': 'ㄿ', 'fg': 'ㅀ', 'a': 'ㅁ', 'q': 'ㅂ', 'qt': 'ㅄ', 't': 'ㅅ', 'T': 'ㅆ', 'd': 'ㅇ', 'w': 'ㅈ', 'c': 'ㅊ', 'z': 'ㅋ', 'x': 'ㅌ', 'v': 'ㅍ', 'g': 'ㅎ', 'k': 'ㅏ', 'o': 'ㅐ', 'i': 'ㅑ', 'O': 'ㅒ', 'j': 'ㅓ', 'p': 'ㅔ', 'u': 'ㅕ', 'P': 'ㅖ', 'h': 'ㅗ', 'hk': 'ㅘ', 'ho': 'ㅙ', 'hl': 'ㅚ', 'y': 'ㅛ', 'n': 'ㅜ', 'nj': 'ㅝ', 'np': 'ㅞ', 'nl': 'ㅟ', 'b': 'ㅠ', 'm': 'ㅡ', 'ml': 'ㅢ', 'l': 'ㅣ'}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "KORS = tuple(\"ㄱㄲㄳㄴㄵㄶㄷㄹㄺㄻㄼㄽㄾㄿㅀㅁㅂㅄㅅㅆㅇㅈㅊㅋㅌㅍㅎㅏㅐㅑㅒㅓㅔㅕㅖㅗㅘㅙㅚㅛㅜㅝㅞㅟㅠㅡㅢㅣ\")\n",
    "ENGS = (\"r\", \"R\", \"rt\", \"s\", \"sw\", \"sg\", \"e\", \"f\", \"fr\", \"fa\", \"fq\", \"ft\", \"fx\", \"fv\", \"fg\", \"a\", \"q\", \"qt\", \"t\",\n",
    "        \"T\", \"d\", \"w\", \"c\", \"z\", \"x\", \"v\", \"g\", \"k\", \"o\", \"i\", \"O\", \"j\", \"p\", \"u\", \"P\", \"h\", \"hk\", \"ho\", \"hl\", \"y\",\n",
    "        \"n\", \"nj\", \"np\", \"nl\", \"b\", \"m\", \"ml\", \"l\",\n",
    "       )\n",
    "\n",
    "\n",
    "eng_kor = dict(zip(ENGS, KORS))\n",
    "\n",
    "print(eng_kor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4c449f4",
   "metadata": {},
   "source": [
    "영어를 key, 해당 영어 키보드에 해당하는 한글을 value로 하는 dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5e9be5ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ㄱ', 'ㄴ', 'ㄷ', 'ㄹ', 'ㅁ', 'ㅂ', 'ㅅ', 'ㅇ', 'ㅈ', 'ㅊ', 'ㅋ', 'ㅌ', 'ㅍ', 'ㅎ', 'ㅏ', 'ㅐ', 'ㅑ', 'ㅓ', 'ㅔ', 'ㅕ', 'ㅗ', 'ㅛ', 'ㅜ', 'ㅠ', 'ㅡ', 'ㅣ']\n",
      "'ㄱ' is KOR\n",
      "'ㄴ' is KOR\n",
      "'ㄷ' is KOR\n",
      "'ㄹ' is KOR\n",
      "'ㅁ' is KOR\n",
      "'ㅂ' is KOR\n",
      "'ㅅ' is KOR\n",
      "'ㅇ' is KOR\n",
      "'ㅈ' is KOR\n",
      "'ㅊ' is KOR\n",
      "'ㅋ' is KOR\n",
      "'ㅌ' is KOR\n",
      "'ㅍ' is KOR\n",
      "'ㅎ' is KOR\n",
      "'ㅏ' is KOR\n",
      "'ㅐ' is KOR\n",
      "'ㅑ' is KOR\n",
      "'ㅓ' is KOR\n",
      "'ㅔ' is KOR\n",
      "'ㅕ' is KOR\n",
      "'ㅗ' is KOR\n",
      "'ㅛ' is KOR\n",
      "'ㅜ' is KOR\n",
      "'ㅠ' is KOR\n",
      "'ㅡ' is KOR\n",
      "'ㅣ' is KOR\n"
     ]
    }
   ],
   "source": [
    "base_dir = \"./dataset/captures\"\n",
    "folders = os.listdir(base_dir)\n",
    "print(folders)\n",
    "\n",
    "for folder in folders:\n",
    "    try:\n",
    "        src = os.path.join(base_dir, folder)\n",
    "        dst = os.path.join(base_dir, eng_kor[folder])\n",
    "        os.rename(src, dst)\n",
    "        print(src + \" to \" + dst)\n",
    "    except KeyError as e:\n",
    "        print(e, \"is KOR\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e521e24b",
   "metadata": {},
   "source": [
    "위에서 eng_kor를 사용해 모든 카테고리 폴더를 영어에서 한글로 변환\n",
    "\n",
    "폴더 이름이 이미 한글일 시 해당 폴더 명과 문구 출력"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43ff035b",
   "metadata": {},
   "source": [
    "> handKeypoint.py\n",
    "\n",
    "이미지 데이터에서 아래 사진과 같이 총 21개 keypoint들의 x, y, z 좌표를 추출해 npy 파일로 저장\n",
    "\n",
    "keypoint 추출에는 mediapipe 라이브러리 사용\n",
    "![hand keypoint](https://google.github.io/mediapipe/images/mobile/hand_landmarks.png)\n",
    "손 위치별 좌표 / https://google.github.io/mediapipe/images/mobile/hand_landmarks.png"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0a7c63ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ㄱ', 'ㄴ', 'ㄷ', 'ㄹ', 'ㅁ', 'ㅂ', 'ㅅ', 'ㅇ', 'ㅈ', 'ㅊ', 'ㅋ', 'ㅌ', 'ㅍ', 'ㅎ', 'ㅏ', 'ㅐ', 'ㅑ', 'ㅓ', 'ㅔ', 'ㅕ', 'ㅗ', 'ㅛ', 'ㅜ', 'ㅠ', 'ㅡ', 'ㅣ']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from glob import glob\n",
    "\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_hands = mp.solutions.hands\n",
    "\n",
    "base_dir = \"./dataset/captures\"\n",
    "folders = os.listdir(base_dir)\n",
    "print(folders)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "041cde2d",
   "metadata": {},
   "source": [
    "mediapipe를 사용하기 위한 모듈을 가져오고 변환할 폴더 지정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c883b32",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for folder in folders:\n",
    "    IMAGE_FILES = glob(os.path.join(base_dir, folder, \"*.*\"))\n",
    "    print(IMAGE_FILES)\n",
    "    with mp_hands.Hands(\n",
    "        static_image_mode=True, max_num_hands=2, min_detection_confidence=0.5\n",
    "    ) as hands:\n",
    "        for idx, file in enumerate(IMAGE_FILES):\n",
    "            i = idx + 1\n",
    "            print(i, file)\n",
    "            ff = np.fromfile(file, np.uint8)\n",
    "            image = cv2.imdecode(ff, cv2.IMREAD_UNCHANGED)\n",
    "            # print(image)\n",
    "            # print(file)\n",
    "            # Convert the BGR image to RGB before processing.\n",
    "            results = hands.process(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
    "            xyz = []\n",
    "            if results.multi_hand_landmarks:\n",
    "                for landmark in results.multi_hand_landmarks[0].landmark:\n",
    "                    xyz.append([landmark.x, landmark.y, landmark.z])\n",
    "\n",
    "                # Print handedness and draw hand landmarks on the image.\n",
    "                if not os.path.exists(os.path.join(\"./dataset/keypoints\", folder)):\n",
    "                    os.mkdir(os.path.join(\"./dataset/keypoints\", folder))\n",
    "                np.save(f\"./dataset/keypoints/{folder}/{i}\", xyz)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64d147e4",
   "metadata": {},
   "source": [
    "* 실행 결과 일부  \n",
    "110 ./dataset/captures\\ㅣ\\83.png  \n",
    "111 ./dataset/captures\\ㅣ\\84.png  \n",
    "112 ./dataset/captures\\ㅣ\\85.png  \n",
    "113 ./dataset/captures\\ㅣ\\86.png  \n",
    "114 ./dataset/captures\\ㅣ\\87.png  \n",
    "115 ./dataset/captures\\ㅣ\\88.png  \n",
    "116 ./dataset/captures\\ㅣ\\89.png  \n",
    "117 ./dataset/captures\\ㅣ\\9.png  \n",
    "118 ./dataset/captures\\ㅣ\\90.png  \n",
    "119 ./dataset/captures\\ㅣ\\91.png  \n",
    "120 ./dataset/captures\\ㅣ\\92.png  \n",
    "121 ./dataset/captures\\ㅣ\\93.png  \n",
    "122 ./dataset/captures\\ㅣ\\94.png  \n",
    "123 ./dataset/captures\\ㅣ\\95.png  \n",
    "124 ./dataset/captures\\ㅣ\\96.png  \n",
    "125 ./dataset/captures\\ㅣ\\97.png  \n",
    "126 ./dataset/captures\\ㅣ\\98.png  \n",
    "127 ./dataset/captures\\ㅣ\\99.png"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f0af14e",
   "metadata": {},
   "source": [
    "Hands 객체를 생성하고 각 폴더 내 파일마다\n",
    "```python\n",
    "ff = np.fromfile(file, np.uint8)\n",
    "image = cv2.imdecode(ff, cv2.IMREAD_UNCHANGED)\n",
    "```\n",
    "코드로 이미지를 하나씩 받아옴. 위 코드는 폴더명이 한글일 때 cv2로 이미지를 불러오기 위한 코드\n",
    "\n",
    "```python\n",
    "results = hands.process(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
    "```\n",
    "cv2는 이미지가 BGR 순서이기 때문에 cvtColor를 사용해 RGB 순서로 변환한 뒤 Hands 객체의 process 메소드를 사용해 손의 21개 keypoint를 검출  \n",
    "검출 결과 results에 저장\n",
    "\n",
    "```python\n",
    "xyz = []\n",
    "if results.multi_hand_landmarks:\n",
    "    for landmark in results.multi_hand_landmarks[0].landmark:\n",
    "        xyz.append([landmark.x, landmark.y, landmark.z])\n",
    "\n",
    "    # Print handedness and draw hand landmarks on the image.\n",
    "    if not os.path.exists(os.path.join(\"./dataset/keypoints\", folder)):\n",
    "        os.mkdir(os.path.join(\"./dataset/keypoints\", folder))\n",
    "    np.save(f\"./dataset/keypoints/{folder}/{i}\", xyz)\n",
    "```\n",
    "results.multi_hand_landmarks가 값이 있을경우 즉, 이미지 파일에서 손이 검출되었을 경우 21개 landmark마다 x, y, z 좌표를 추출해 리스트로 저장  \n",
    "해당 리스트의 shape: (21, 3)  \n",
    "해당 리스트는 keypoints 안의 각 지문자 폴더에 npy 형식으로 저장"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aca612ad",
   "metadata": {},
   "source": [
    "## 2. 모델링"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ecd99e3",
   "metadata": {},
   "source": [
    "> modeling.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e14c8ef3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "from keras.callbacks import History\n",
    "from keras.layers import Activation, Conv2D, Dense, Dropout, Flatten, Input, MaxPooling2D\n",
    "from keras.models import Sequential\n",
    "from keras.preprocessing.image import img_to_array, load_img\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "13579c57",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KSLtoText:\n",
    "    def __init__(self):\n",
    "        self.data = []\n",
    "        self.model = Sequential()\n",
    "        self.history = History()\n",
    "        self.dense_size = 0\n",
    "        self.epochs = 0\n",
    "        self.batch_size = 0\n",
    "        self.npy = False\n",
    "\n",
    "    def set_train_test(self, categories, base_dir, img_size=100, npy=False):\n",
    "        self.npy = npy\n",
    "        X = []\n",
    "        Y = []\n",
    "        self.dense_size = len(categories)\n",
    "        for index, cat in enumerate(categories):\n",
    "            # print(index,cat)\n",
    "            files = glob.glob(os.path.join(base_dir, cat, \"*.*\"))\n",
    "            # print(files)\n",
    "            for f in files:\n",
    "                if not npy:\n",
    "                    img = img_to_array(\n",
    "                        load_img(f, color_mode=\"rgb\", target_size=(img_size, img_size))\n",
    "                    )\n",
    "                else:\n",
    "                    img = np.load(f)\n",
    "                X.append(img)\n",
    "                Y.append(index)\n",
    "        X = np.asarray(X)\n",
    "        Y = np.asarray(Y)\n",
    "        if not npy:\n",
    "            X = X.astype(\"float32\") / 255.0\n",
    "        Y = to_categorical(Y, self.dense_size)\n",
    "        self.data = train_test_split(X, Y, test_size=0.2, random_state=1)\n",
    "        print(self.data[3])\n",
    "\n",
    "    def set_model(self):\n",
    "        if self.npy:\n",
    "            self.model.add(Input(shape=self.data[0].shape[1:]))\n",
    "        else:\n",
    "            self.model.add(Conv2D(100, (3, 3), padding=\"same\", input_shape=self.data[0].shape[1:]))\n",
    "            self.model.add(Activation(\"relu\"))\n",
    "            self.model.add(Conv2D(64, (3, 3)))\n",
    "            self.model.add(Activation(\"relu\"))\n",
    "            self.model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "            self.model.add(Dropout(0.25))\n",
    "\n",
    "            self.model.add(Conv2D(64, (3, 3), padding=\"same\"))\n",
    "            self.model.add(Activation(\"relu\"))\n",
    "            self.model.add(Conv2D(64, (3, 3)))\n",
    "            self.model.add(Activation(\"relu\"))\n",
    "            self.model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "            self.model.add(Dropout(0.25))\n",
    "\n",
    "        self.model.add(Flatten())\n",
    "        self.model.add(Dense(512))\n",
    "        self.model.add(Activation(\"relu\"))\n",
    "        self.model.add(Dropout(0.5))\n",
    "        self.model.add(Dense(self.dense_size))\n",
    "        self.model.add(Activation(\"softmax\"))\n",
    "\n",
    "        self.model.summary()\n",
    "\n",
    "    def train_model(self, epochs, batch_size):\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.model.compile(loss=\"categorical_crossentropy\", optimizer=\"Adam\", metrics=[\"accuracy\"])\n",
    "        self.history = self.model.fit(\n",
    "            self.data[0],\n",
    "            self.data[2],\n",
    "            batch_size=batch_size,\n",
    "            epochs=epochs,\n",
    "            validation_data=(self.data[1], self.data[3]),\n",
    "        )\n",
    "\n",
    "    def predict_save(self, filename):\n",
    "        predict_classes = self.model.predict_classes(self.data[1])\n",
    "        # prob = self.model.predict_proba(self.data[1])\n",
    "\n",
    "        self.model.save(f\"./models/{filename}_epochs-{self.epochs}_batch-{self.batch_size}.hdf5\")\n",
    "        predict_classes = self.model.predict_classes(self.data[1], batch_size=5)\n",
    "        true_classes = np.argmax(self.data[3], 1)\n",
    "\n",
    "        print(confusion_matrix(true_classes, predict_classes))\n",
    "\n",
    "        print(self.model.evaluate(self.data[1], self.data[3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11e011c7",
   "metadata": {},
   "source": [
    "`__init__(self)`: 생성된 인스턴스가 기억하고 있어야 할 내용들을 초기화한다.\n",
    "\n",
    "`set_train_test(self, categories, base_dir, img_size=100, npy=False)`: `npy`가 False일 경우 `base_dir` 내의 `category`에 해당하는 폴더별 이미지들을 ndarray로 만들어 객체 내 `data` 변수에 저장한다. `npy`가 true일 경우 `base_dir` 내의 `category`에 해당하는 폴더별 `.npy` 데이터들을 로드해 객체 내 `data` 변수에 저장한다.\n",
    "\n",
    "`set_model(self)`: 객체의 `npy` 변수가 true일 경우 CNN 대신 일반적인 DNN을 사용하고 false일 경우 DNN 대신 CNN을 앞에 추가한다.\n",
    "\n",
    "`train_model(self, epochs, batch_size)`: 객체에 저장된 모델을 설정한 `epochs`, `batch_size`만큼 학습\n",
    "\n",
    "`predict_save(self, filename)`: 객체에 저장되어있는 모델을 `.hdf5` 파일로 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42a2d53b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ktt = KSLtoText()\n",
    "categories = [\"ㄱ\", \"ㄴ\", \"ㄷ\", \"ㄹ\", \"ㅁ\", \"ㅂ\", \"ㅅ\", \"ㅇ\",\n",
    "              \"ㅈ\", \"ㅊ\", \"ㅋ\", \"ㅌ\", \"ㅍ\", \"ㅎ\", \"ㅏ\", \"ㅐ\",\n",
    "              \"ㅑ\", \"ㅓ\", \"ㅔ\", \"ㅕ\", \"ㅗ\", \"ㅛ\", \"ㅜ\", \"ㅠ\", \"ㅡ\", \"ㅣ\",\n",
    "             ]\n",
    "print(len(categories))\n",
    "# ktt.set_train_test(categories, \"./dataset/captures\")\n",
    "ktt.set_train_test(categories, \"./dataset/keypoints\", npy=True)\n",
    "print(ktt.data[0].shape[1:])\n",
    "print(ktt.data[0][0])\n",
    "ktt.set_model()\n",
    "ktt.train_model(epochs=500, batch_size=10)\n",
    "ktt.predict_save(\"ksl-keypoints\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13bbd7ba",
   "metadata": {},
   "source": [
    "* 실행 결과 일부  \n",
    "Epoch 490/500\n",
    "249/249 [==============================] - 0s 873us/step - loss: 0.0428 - accuracy: 0.9819 - val_loss: 0.1347 - val_accuracy: 0.9839  \n",
    "Epoch 491/500\n",
    "249/249 [==============================] - 0s 903us/step - loss: 0.0384 - accuracy: 0.9839 - val_loss: 0.1205 - val_accuracy: 0.9823  \n",
    "Epoch 492/500\n",
    "249/249 [==============================] - 0s 933us/step - loss: 0.0483 - accuracy: 0.9807 - val_loss: 0.1862 - val_accuracy: 0.9710  \n",
    "Epoch 493/500\n",
    "249/249 [==============================] - 0s 861us/step - loss: 0.0539 - accuracy: 0.9778 - val_loss: 0.1238 - val_accuracy: 0.9823  \n",
    "Epoch 494/500\n",
    "249/249 [==============================] - 0s 857us/step - loss: 0.0651 - accuracy: 0.9750 - val_loss: 0.1309 - val_accuracy: 0.9791  \n",
    "Epoch 495/500\n",
    "249/249 [==============================] - 0s 845us/step - loss: 0.0573 - accuracy: 0.9774 - val_loss: 0.1175 - val_accuracy: 0.9807  \n",
    "Epoch 496/500\n",
    "249/249 [==============================] - 0s 977us/step - loss: 0.0441 - accuracy: 0.9847 - val_loss: 0.1367 - val_accuracy: 0.9823  \n",
    "Epoch 497/500\n",
    "249/249 [==============================] - 0s 831us/step - loss: 0.0417 - accuracy: 0.9819 - val_loss: 0.1386 - val_accuracy: 0.9823  \n",
    "Epoch 498/500\n",
    "249/249 [==============================] - 0s 961us/step - loss: 0.0393 - accuracy: 0.9827 - val_loss: 0.1489 - val_accuracy: 0.9791  \n",
    "Epoch 499/500\n",
    "249/249 [==============================] - 0s 839us/step - loss: 0.0374 - accuracy: 0.9863 - val_loss: 0.1621 - val_accuracy: 0.9839  \n",
    "Epoch 500/500\n",
    "249/249 [==============================] - 0s 825us/step - loss: 0.0574 - accuracy: 0.9811 - val_loss: 0.1450 - val_accuracy: 0.9839  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f327f2f",
   "metadata": {},
   "source": [
    "KSLtoText 객체 생성 후 모델 학습  \n",
    "이미지 데이터를 사용해 학습했을 경우 정확도는 높게 나왔으나 배경제거 등 별다른 전처리 없이 학습시켰기 때문에 opencv를 사용해 검증했을 때 정확도가 매우 낮았음.  \n",
    "keypoint 데이터를 사용해 학습했을 땐 정확도는 물론 opencv를 사용한 검증도 높은 성능을 보임"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab4a0cc5",
   "metadata": {},
   "source": [
    "## 3. 시각화"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5bc9d82",
   "metadata": {},
   "source": [
    "> KSLRecognizer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f8f37c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "from keras.models import load_model\n",
    "from keras.preprocessing import image\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_hands = mp.solutions.hands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ff2e8ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_text(image, string):\n",
    "    temp = Image.fromarray(image)\n",
    "    draw = ImageDraw.Draw(temp)\n",
    "    font = ImageFont.truetype(\"fonts/gulim.ttc\", 20)\n",
    "    draw.text(\n",
    "        (image.shape[0] / 7, image.shape[1] / 5),\n",
    "        string,\n",
    "        font=font,\n",
    "        fill=(255, 0, 0),\n",
    "        # stroke_width=2,\n",
    "    )\n",
    "    image = np.array(temp)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9e2e53f",
   "metadata": {},
   "source": [
    "이미지에 문자열을 넣는 함수  \n",
    "`cv2.putText`의 경우 한글을 지원하지 않기 때문에 PIL을 사용해 텍스트를 삽입한다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bf8be8ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model(\"./models/ksl-keypoints_epochs-500_batch-10.hdf5\")\n",
    "categories = [\"ㄱ\", \"ㄴ\", \"ㄷ\", \"ㄹ\", \"ㅁ\", \"ㅂ\", \"ㅅ\", \"ㅇ\",\n",
    "              \"ㅈ\", \"ㅊ\", \"ㅋ\", \"ㅌ\", \"ㅍ\", \"ㅎ\", \"ㅏ\", \"ㅐ\",\n",
    "              \"ㅑ\", \"ㅓ\", \"ㅔ\", \"ㅕ\", \"ㅗ\", \"ㅛ\", \"ㅜ\", \"ㅠ\", \"ㅡ\", \"ㅣ\",\n",
    "             ]\n",
    "string = \"loading\"\n",
    "count = 0\n",
    "cap = cv2.VideoCapture(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01977af8",
   "metadata": {},
   "source": [
    "모델 로드 및 기타 상수 선언"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "11f7e583",
   "metadata": {},
   "outputs": [],
   "source": [
    "with mp_hands.Hands(\n",
    "        min_detection_confidence=0.5, min_tracking_confidence=0.5, max_num_hands=1\n",
    "    ) as hands:\n",
    "    while cap.isOpened():\n",
    "        success, image = cap.read()\n",
    "        if not success:\n",
    "            print(\"Ignoring empty camera frame.\")\n",
    "            continue\n",
    "\n",
    "        # Flip the image horizontally for a later selfie-view display, and convert\n",
    "        # the BGR image to RGB.\n",
    "        image = cv2.cvtColor(cv2.flip(image, 1), cv2.COLOR_BGR2RGB)\n",
    "        # To improve performance, optionally mark the image as not writeable to\n",
    "        # pass by reference.\n",
    "        image.flags.writeable = False\n",
    "        roi = image[100:300, 200:400].copy()\n",
    "        image = cv2.rectangle(image, (200, 100), (400, 300), (255, 0, 0), 5, cv2.LINE_8)\n",
    "        img_for_process = roi.copy()\n",
    "        img_for_process = cv2.resize(img_for_process, (100, 100))\n",
    "\n",
    "        results = hands.process(img_for_process)\n",
    "\n",
    "        # Draw the hand annotations on the image.\n",
    "        image.flags.writeable = True\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "        black = np.zeros((200, 200, 3), np.uint8)\n",
    "\n",
    "        # 검정 창에 keypoints 출력\n",
    "        if results.multi_hand_landmarks:\n",
    "            for hand_landmarks in results.multi_hand_landmarks:\n",
    "                mp_drawing.draw_landmarks(black, hand_landmarks, mp_hands.HAND_CONNECTIONS)\n",
    "        count += 1\n",
    "        if count == 10:\n",
    "            count = 0\n",
    "            xyz = []\n",
    "            if results.multi_hand_landmarks:\n",
    "                for landmark in results.multi_hand_landmarks[0].landmark:\n",
    "                    xyz.append([landmark.x, landmark.y, landmark.z])\n",
    "                xyz = np.expand_dims(xyz, axis=0)\n",
    "                xyz = np.asarray(xyz)\n",
    "                xyz = xyz / 255.0\n",
    "                print(xyz)\n",
    "                prob = model.predict_proba(xyz)\n",
    "                print(\"Predicted:\")\n",
    "                # print(prob)\n",
    "                print(np.max(prob))\n",
    "                classes = np.argmax(model.predict(xyz), axis=-1)\n",
    "                # print(classes)\n",
    "                print(categories[classes[0]])\n",
    "                string = categories[classes[0]] + f\"   {np.max(prob)*100:.2f}\"\n",
    "        image = add_text(image, string)\n",
    "\n",
    "        cv2.imshow(\"Hands\", image)\n",
    "        cv2.imshow(\"keypoints\", black)\n",
    "\n",
    "        if cv2.waitKey(5) & 0xFF == 27:\n",
    "            break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e392c27d",
   "metadata": {},
   "source": [
    "mediapipe를 사용해 카메라에서 손의 keypoint들을 검출하고, 모델에 적용시켜 얻은 예측 값을 이미지에 표시함"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2ebd832",
   "metadata": {},
   "source": [
    "##### 실행 화면 넣으면 딱인 위치"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a0a79390e55e9f153ee009263159081774ee90ddfe589903187d8d1fd7bd3736"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
